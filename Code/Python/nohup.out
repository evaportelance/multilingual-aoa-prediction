log_dir: ../../Results/experiments/2021-12-10_lstm_eng_20e_256b_em100_hd100_v5000_run0/tensorboard-log
0
train acc: 0.9726970290694084
loss :0.2804566868316237
1
train acc: 0.9978728088995402
loss :0.017213404641719637
2
train acc: 0.9999691497493862
loss :0.0010690452645616935
3
train acc: 0.9999992116316601
loss :9.448460645937909e-05
4
train acc: 0.9999995452979932
loss :2.014406614041339e-05
5
train acc: 0.9999996219176697
loss :8.154059798891291e-06
6
train acc: 0.9999996540432984
loss :5.096856330690921e-06
7
train acc: 0.9999996688729131
loss :3.9340968035236295e-06
8
train acc: 0.9999997034701443
loss :3.296560931028835e-06
9
train acc: 0.9999997182945558
loss :2.8806367408547637e-06
10
train acc: 0.9999997430105805
loss :2.5458193238985585e-06
11
train acc: 0.999999752891787
loss :2.2759421235450434e-06
12
train acc: 0.9999997553633895
loss :2.059977239836048e-06
13
train acc: 0.9999997800742108
loss :1.8434643758430797e-06
14
train acc: 0.9999997973702246
loss :1.6754430514243636e-06
15
train acc: 0.9999998319674558
loss :1.5118985795808223e-06
16
train acc: 0.9999998616162786
loss :1.3578166865667679e-06
17
train acc: 0.9999998764458935
loss :1.2310355127716314e-06
18
train acc: 0.9999999135095238
loss :1.0699471145777243e-06
19
train acc: 0.9999999357487426
loss :8.771872446298704e-07
epoch: 1
test_accuracy: 0.9999983626842645
log_dir: ../../Results/experiments/2021-12-10_lstm_eng_20e_256b_em100_hd100_v5000_run1/tensorboard-log
0
train acc: 0.9711800557694586
loss :0.2956365218177106
1
train acc: 0.9977852310586943
loss :0.017628975197614305
2
train acc: 0.9999657639766145
loss :0.0011270329089520645
3
train acc: 0.9999994809686852
loss :9.665015689778254e-05
4
train acc: 0.9999998863062864
loss :1.8035504350699695e-05
5
train acc: 0.9999999530395531
loss :6.155573426766564e-06
6
train acc: 0.9999999703407704
loss :3.2290334995008357e-06
7
train acc: 0.9999999752839753
loss :2.126672619401706e-06
8
train acc: 0.9999999802271803
loss :1.5907124536935546e-06
9
train acc: 0.9999999851703852
loss :1.2899185146686033e-06
10
train acc: 0.9999999901135901
loss :1.0478476551046335e-06
11
train acc: 0.9999999901135901
loss :9.050256186882411e-07
12
train acc: 0.9999999901135901
loss :7.947914169012642e-07
13
train acc: 0.9999999901135901
loss :7.09416820382718e-07
14
train acc: 0.9999999901135901
loss :6.439719984933103e-07
15
train acc: 0.9999999901135901
loss :5.866006172832471e-07
16
train acc: 0.9999999901135901
loss :5.426376388094821e-07
17
train acc: 0.9999999901135901
loss :5.01552695064553e-07
18
train acc: 0.9999999901135901
loss :4.6772950636617274e-07
19
train acc: 0.9999999925851926
loss :4.382521102700052e-07
epoch: 1
test_accuracy: 0.9999948897348644
log_dir: ../../Results/experiments/2021-12-10_lstm_eng_20e_256b_em100_hd100_v5000_run2/tensorboard-log
0
train acc: 0.971406343123076
loss :0.29492328165920995
1
train acc: 0.9978643493900025
loss :0.017537333214975036
2
train acc: 0.9999647580708337
loss :0.0011885385334309611
3
train acc: 0.9999993599017912
loss :0.00011207624482879572
4
train acc: 0.9999997232169472
loss :2.360020543435212e-05
5
train acc: 0.9999999283287319
loss :8.348431169908674e-06
6
train acc: 0.9999999703407704
loss :4.023794834398381e-06
7
train acc: 0.9999999752839753
loss :2.560223237701193e-06
8
train acc: 0.9999999777555778
loss :1.8555132752680352e-06
9
train acc: 0.9999999777555778
loss :1.4556279964501137e-06
10
train acc: 0.9999999826987828
loss :1.196486656710551e-06
11
train acc: 0.9999999851703852
loss :1.0169905113216467e-06
12
train acc: 0.9999999851703852
loss :8.921684076930695e-07
13
train acc: 0.9999999876419876
loss :7.913701174457954e-07
14
train acc: 0.9999999901135901
loss :7.097408116930491e-07
15
train acc: 0.9999999901135901
loss :6.487095075655796e-07
16
train acc: 0.9999999901135901
loss :5.928422671928514e-07
17
train acc: 0.9999999901135901
loss :5.474457186364233e-07
18
train acc: 0.9999999901135901
loss :5.08897947997102e-07
19
train acc: 0.9999999901135901
loss :4.764641820083587e-07
epoch: 1
test_accuracy: 0.9999966206916275
log_dir: ../../Results/experiments/2021-12-10_lstm_fra_20e_256b_em100_hd100_v5000_run0/tensorboard-log
0
train acc: 0.9278100394493785
loss :1.4132638913775804
1
train acc: 0.9558609394852174
loss :0.3878349934149226
2
train acc: 0.9613209916364163
loss :0.2932744577390338
3
train acc: 0.9632280254035914
loss :0.2468872630268062
4
train acc: 0.9671674249915901
loss :0.20388365579307627
5
train acc: 0.973264431570648
loss :0.16645826987170298
6
train acc: 0.9800011407891545
loss :0.1307480278365109
7
train acc: 0.9861431018474999
loss :0.1013819493285013
8
train acc: 0.9894345288429786
loss :0.07869390785147291
9
train acc: 0.9916956950765137
loss :0.061533992662342316
10
train acc: 0.9934115721545088
loss :0.04811691844135249
11
train acc: 0.9949379000641884
loss :0.03734169006347656
12
train acc: 0.9962175145061738
loss :0.02872300979194291
13
train acc: 0.9972046390039111
loss :0.021974838326830382
14
train acc: 0.9979423291639451
loss :0.016752848493943522
15
train acc: 0.99850731032704
loss :0.012735029098090775
16
train acc: 0.9989476549516031
loss :0.009691830731313163
17
train acc: 0.9992948042143376
loss :0.00736664325819103
18
train acc: 0.9995489425615434
loss :0.005585464862508512
19
train acc: 0.9997275080155889
loss :0.004225502976583778
epoch: 1
test_accuracy: 0.9989724010687608
log_dir: ../../Results/experiments/2021-12-10_lstm_deu_20e_256b_em100_hd100_v5000_run0/tensorboard-log
0
train acc: 0.8891431555247251
loss :1.7967997978703099
1
train acc: 0.9151843630206522
loss :0.7812706113485766
2
train acc: 0.921395356170378
loss :0.5669434765454571
3
train acc: 0.9283794458519934
loss :0.4748066474421902
4
train acc: 0.9341658958693866
loss :0.4028399402558491
5
train acc: 0.9450230705072327
loss :0.33341537928289977
6
train acc: 0.9595615755096988
loss :0.2659937940134844
7
train acc: 0.9706420131794862
loss :0.20673428946556638
8
train acc: 0.9781160362415913
loss :0.15818448424547338
9
train acc: 0.9840831716023191
loss :0.1192903502033227
10
train acc: 0.9884438999452309
loss :0.08804051663863097
11
train acc: 0.9920430074707584
loss :0.06309244978074212
12
train acc: 0.9947688986285611
loss :0.04400642088778564
13
train acc: 0.9967625162988433
loss :0.03016147979683069
14
train acc: 0.9981888534198881
loss :0.02037171335536564
15
train acc: 0.9991887666376057
loss :0.013568578590273233
16
train acc: 0.9996997300034834
loss :0.0089247963191327
17
train acc: 0.9998993113194877
loss :0.00580805758531181
18
train acc: 0.9999659570619907
loss :0.003763485121269293
19
train acc: 0.9999888826415176
loss :0.0024399145735496002
epoch: 1
test_accuracy: 0.99988606163265
log_dir: ../../Results/experiments/2021-12-10_lstm_spa_20e_256b_em100_hd100_v5000_run0/tensorboard-log
0
train acc: 0.7871313017968469
loss :3.9321533912836117
1
train acc: 0.8753807480598605
loss :1.4837346631427144
2
train acc: 0.8753798091134359
loss :1.3433841439180596
3
train acc: 0.8753778482245844
loss :1.270643722179324
4
train acc: 0.8753790987092395
loss :1.210787484812182
5
train acc: 0.8753815544552581
loss :1.1469469292219294
6
train acc: 0.8753797283699346
loss :1.0299623178881268
7
train acc: 0.875379946689273
loss :0.8522643155829851
8
train acc: 0.8749961513419484
loss :0.7679575543070949
9
train acc: 0.8839075353949569
loss :0.7141159412472747
10
train acc: 0.8912236179376758
loss :0.671159123265466
11
train acc: 0.8974726816942525
loss :0.637424158495526
12
train acc: 0.9002992328862811
loss :0.6082898516987645
13
train acc: 0.9033757446463718
loss :0.5816193957661473
14
train acc: 0.9077482176728027
loss :0.5564576526020848
15
train acc: 0.9118294265381125
loss :0.53139393828636
16
train acc: 0.9163048577516578
loss :0.5056194482847701
17
train acc: 0.9197458701078282
loss :0.47881024382835213
18
train acc: 0.9236583685459092
loss :0.4511648222457531
19
train acc: 0.9265572621032249
loss :0.423271844553393
epoch: 1
test_accuracy: 0.9588162328543837
log_dir: ../../Results/experiments/2021-12-10_lstm_zho_20e_256b_em100_hd100_v5000_run0/tensorboard-log
0
train acc: 0.8232925141269968
loss :2.708980334051724
1
train acc: 0.8658429737748771
loss :1.3793585836476292
2
train acc: 0.8658406255162996
loss :1.1154109981142242
3
train acc: 0.8750094953898726
loss :0.8670457300646551
4
train acc: 0.8801384446538728
loss :0.7561689284752156
5
train acc: 0.8881809584847812
loss :0.6799562230603449
6
train acc: 0.8947922610414439
loss :0.6252933896821121
7
train acc: 0.9062580437495791
loss :0.5693746632543103
8
train acc: 0.916869902939632
loss :0.5065818628771551
9
train acc: 0.928840080622969
loss :0.4400639816810345
10
train acc: 0.941628497715654
loss :0.3765862826643319
11
train acc: 0.9530026902823613
loss :0.3166382256869612
12
train acc: 0.9631895531457046
loss :0.2621227816877694
13
train acc: 0.9701996349466258
loss :0.2150054931640625
14
train acc: 0.9769065626736345
loss :0.17494477370689654
15
train acc: 0.9821058473093757
loss :0.1411193111025054
16
train acc: 0.9862219094407969
loss :0.11290325296336207
17
train acc: 0.9893584719197503
loss :0.08992762072332974
18
train acc: 0.9916760684703958
loss :0.07140328769026132
19
train acc: 0.9934838431456994
loss :0.056782947408741916
epoch: 1
test_accuracy: 0.9963132272898455
log_dir: ../../Results/experiments/2021-12-10_lstm_fra_20e_256b_em100_hd100_v5000_run1/tensorboard-log
0
train acc: 0.9197165834514793
loss :1.487244548272649
1
train acc: 0.9557280248458232
loss :0.39841680964198684
2
train acc: 0.9592841986669313
loss :0.2999459082927179
3
train acc: 0.9617606526667919
loss :0.25656861471473624
4
train acc: 0.9644959695842288
loss :0.21437393328465454
5
train acc: 0.9704925512501953
loss :0.17825629558038275
6
train acc: 0.9772958238190467
loss :0.14220485862241972
7
train acc: 0.9837479056568321
loss :0.11139136498127508
8
train acc: 0.9885023487270426
loss :0.08588608208052609
9
train acc: 0.9910690763674744
loss :0.06628935927644782
10
train acc: 0.9929221231456197
loss :0.05179132934010357
11
train acc: 0.9945534158737288
loss :0.04049115312208823
12
train acc: 0.9958325902803229
loss :0.03137753337895104
13
train acc: 0.996880739271094
loss :0.024089603249086152
14
train acc: 0.9977249535945577
loss :0.01835623399926982
15
train acc: 0.9983874403555458
loss :0.013922644973894872
16
train acc: 0.9988783882298601
loss :0.010522064593953823
17
train acc: 0.9992425548374106
loss :0.007912605180652864
18
train acc: 0.9995156091834427
loss :0.005930623220741202
19
train acc: 0.9997174532041637
loss :0.004426832811548076
epoch: 1
test_accuracy: 0.9988932209748488
log_dir: ../../Results/experiments/2021-12-10_lstm_deu_20e_256b_em100_hd100_v5000_run1/tensorboard-log
0
train acc: 0.8833894731981801
loss :1.8498765236420156
1
train acc: 0.9151824929119197
loss :0.8083673979807483
2
train acc: 0.9185947204239497
loss :0.568832850164976
3
train acc: 0.9257861984337812
loss :0.4747528555505563
4
train acc: 0.9318621505513449
loss :0.4140544045866056
5
train acc: 0.9403100344523085
loss :0.358631393672284
6
train acc: 0.9522825083703479
loss :0.296139782011821
7
train acc: 0.9651025164397807
loss :0.23236669836660123
8
train acc: 0.9741111385780687
loss :0.1790529554010894
9
train acc: 0.9811202163367579
loss :0.13710938032592987
10
train acc: 0.9862108148413387
loss :0.10367868500022157
11
train acc: 0.9901132600261695
loss :0.07661534520760048
12
train acc: 0.9932929036088847
loss :0.05526293676353042
13
train acc: 0.9956658376969176
loss :0.03912370159155827
14
train acc: 0.9972877986664131
loss :0.027301284031094056
15
train acc: 0.9985279267162969
loss :0.018816982888426457
16
train acc: 0.999314993879974
loss :0.012831199439616312
17
train acc: 0.9997127540864662
loss :0.008665410219894861
18
train acc: 0.9998845006054816
loss :0.0058162075062696845
19
train acc: 0.9999566680993918
loss :0.0039015844022208274
epoch: 1
test_accuracy: 0.9997292891924587
log_dir: ../../Results/experiments/2021-12-10_lstm_spa_20e_256b_em100_hd100_v5000_run1/tensorboard-log
0
train acc: 0.7677389084178157
loss :4.098372348519259
1
train acc: 0.8753806588261627
loss :1.4724737655284792
2
train acc: 0.8753810611575149
loss :1.3355848622876545
3
train acc: 0.8753783404827118
loss :1.268508378849473
4
train acc: 0.8753751720799956
loss :1.2200891361680142
5
train acc: 0.8753816435156867
loss :1.1785658015761264
6
train acc: 0.8753815551483354
loss :1.1234666691269986
7
train acc: 0.8753769562341446
loss :1.021704030591388
8
train acc: 0.8753810585584751
loss :0.8695866784384084
9
train acc: 0.8751346524371657
loss :0.7698864604151526
10
train acc: 0.8814394477494928
loss :0.7143404317456622
11
train acc: 0.8947198475862659
loss :0.6731543873631677
12
train acc: 0.8970923905455789
loss :0.640697434891102
13
train acc: 0.8993280840127967
loss :0.6147414806277253
14
train acc: 0.9010378098072007
loss :0.5898275597150936
15
train acc: 0.9025774679558222
loss :0.5636438768963481
16
train acc: 0.9075024895543276
loss :0.5379144091938817
17
train acc: 0.9125733087922252
loss :0.5127132327057594
18
train acc: 0.9170167085736297
loss :0.48666142308434773
19
train acc: 0.923779368054035
loss :0.4591628229895303
epoch: 1
test_accuracy: 0.9581625919924317
log_dir: ../../Results/experiments/2021-12-10_lstm_zho_20e_256b_em100_hd100_v5000_run1/tensorboard-log
0
train acc: 0.8158311582060435
loss :2.7848969558189656
1
train acc: 0.8658430444783178
loss :1.3817262425915948
2
train acc: 0.8658423079293349
loss :1.2598760775862068
3
train acc: 0.8658410466128382
loss :1.0183448949353449
4
train acc: 0.8765487159531692
loss :0.8258303306842673
5
train acc: 0.8806697200906688
loss :0.7277027209051724
6
train acc: 0.889043534213099
loss :0.6618916268184267
7
train acc: 0.8970968697810995
loss :0.5979119241648707
8
train acc: 0.9079583794495155
loss :0.5296748720366379
9
train acc: 0.9256531133322881
loss :0.4603317365975216
10
train acc: 0.9386372484831974
loss :0.39336632037984914
11
train acc: 0.9478924267045383
loss :0.33342565076104524
12
train acc: 0.956712191680382
loss :0.2820214001885776
13
train acc: 0.9655861528988542
loss :0.23732228246228448
14
train acc: 0.9719213816215252
loss :0.19816016887796337
15
train acc: 0.9776883375233617
loss :0.16413914121430495
16
train acc: 0.9824510058041277
loss :0.13482129327182113
17
train acc: 0.986388690142796
loss :0.10998614081021013
18
train acc: 0.9893658211313445
loss :0.08930661957839439
19
train acc: 0.9914923031576749
loss :0.07231419530408136
epoch: 1
test_accuracy: 0.995077520608902
log_dir: ../../Results/experiments/2021-12-10_lstm_fra_20e_256b_em100_hd100_v5000_run2/tensorboard-log
0
train acc: 0.923518670457143
loss :1.4805785012901376
1
train acc: 0.9557279170653142
loss :0.4379248942803899
2
train acc: 0.9584079731495009
loss :0.3055802091546015
3
train acc: 0.9603994365679015
loss :0.2639145317427609
4
train acc: 0.962720130014857
loss :0.22398207078286267
5
train acc: 0.9650863022432414
loss :0.1971886311102351
6
train acc: 0.9718383148175861
loss :0.16711319389693233
7
train acc: 0.9806067425723469
loss :0.13146195717907827
8
train acc: 0.9866565267427252
loss :0.09872528916105218
9
train acc: 0.9901656880838061
loss :0.07413346176847406
10
train acc: 0.9923983942478075
loss :0.05660964545853641
11
train acc: 0.9942198794916135
loss :0.04351679950679114
12
train acc: 0.9956777534900455
loss :0.033485563085713516
13
train acc: 0.9967718268753192
loss :0.025681583159560456
14
train acc: 0.9975824566062438
loss :0.019650823260666035
15
train acc: 0.9982256642175377
loss :0.015018825356019746
16
train acc: 0.9987495147306985
loss :0.01146923467653607
17
train acc: 0.9991466275048912
loss :0.008746274020693718
18
train acc: 0.9994547442011877
loss :0.006656828714073251
19
train acc: 0.9996673304006594
loss :0.005052688143668918
epoch: 1
test_accuracy: 0.9990003544550675
log_dir: ../../Results/experiments/2021-12-10_lstm_deu_20e_256b_em100_hd100_v5000_run2/tensorboard-log
0
train acc: 0.8847396297082661
loss :1.8387581124563699
1
train acc: 0.9151804312465495
loss :0.8887969278332243
2
train acc: 0.9151808016691324
loss :0.7290233991533049
3
train acc: 0.9215575546078241
loss :0.5206074606477694
4
train acc: 0.9274364406109689
loss :0.4417622351521597
5
train acc: 0.939213200857502
loss :0.3772989109964687
6
train acc: 0.9502430843225116
loss :0.31568107138961604
7
train acc: 0.9597655432282526
loss :0.25812638367657886
8
train acc: 0.9687658258133534
loss :0.20569859582924302
9
train acc: 0.9767093794716174
loss :0.1591090124106948
10
train acc: 0.9833074907461802
loss :0.11918894449869792
11
train acc: 0.988253148854507
loss :0.08663105590181201
12
train acc: 0.9920486009973922
loss :0.06157082496097158
13
train acc: 0.9947995076420836
loss :0.043042580702750056
14
train acc: 0.9967484787288553
loss :0.02965691118756306
15
train acc: 0.9981547170700619
loss :0.02014101737456796
16
train acc: 0.9991414113194531
loss :0.013482274393344634
17
train acc: 0.9996878095426277
loss :0.008917821847539505
18
train acc: 0.9998910207086833
loss :0.005851678199168899
19
train acc: 0.9999621520907883
loss :0.0038319395474738474
epoch: 1
test_accuracy: 0.9995989897294969
log_dir: ../../Results/experiments/2021-12-10_lstm_spa_20e_256b_em100_hd100_v5000_run2/tensorboard-log
0
train acc: 0.7579974904904724
loss :4.061282490575036
1
train acc: 0.875373566219973
loss :1.4768910962481832
2
train acc: 0.8753803481542787
loss :1.3342931880507358
3
train acc: 0.8753788718997046
loss :1.2653936341751453
4
train acc: 0.8753796765624091
loss :1.214677056600881
5
train acc: 0.8753786081838053
loss :1.1701885489530341
6
train acc: 0.875379677602025
loss :1.1075944235158521
7
train acc: 0.8753794100742007
loss :1.0287088793377543
8
train acc: 0.8753738330547199
loss :0.9387955776480741
9
train acc: 0.8753812841551248
loss :0.8688412156215933
10
train acc: 0.8753821742396022
loss :0.8118164594783339
11
train acc: 0.8753776257467825
loss :0.7612805033839026
12
train acc: 0.8884185545666273
loss :0.7005407643872638
13
train acc: 0.8978417203869931
loss :0.6520988996638808
14
train acc: 0.8986579772344855
loss :0.6179917801258176
15
train acc: 0.9003807708274486
loss :0.5908817025118096
16
train acc: 0.9043148769888767
loss :0.5657381013382313
17
train acc: 0.9113389312527901
loss :0.5402507338412973
18
train acc: 0.9170828546548999
loss :0.5131901142209075
19
train acc: 0.9212634930776995
loss :0.4832985900169195
epoch: 1
test_accuracy: 0.9553953540993996
log_dir: ../../Results/experiments/2021-12-10_lstm_zho_20e_256b_em100_hd100_v5000_run2/tensorboard-log
0
train acc: 0.818362210477052
loss :2.771402377424569
1
train acc: 0.8658383444259906
loss :1.3725991716056034
2
train acc: 0.8658406242831
loss :1.207678643588362
3
train acc: 0.8658405899179393
loss :1.0353276535560345
4
train acc: 0.8658416771066599
loss :0.9097102303340517
5
train acc: 0.8658396062357673
loss :0.8455811388739224
6
train acc: 0.8752860860989011
loss :0.8061403050915948
7
train acc: 0.8810911222161918
loss :0.7797284987877156
8
train acc: 0.8815610551011973
loss :0.7571270541487068
9
train acc: 0.8885651840012649
loss :0.7319961442618534
10
train acc: 0.8885653553337887
loss :0.6952169063173491
11
train acc: 0.89261176931447
loss :0.6450588884024785
12
train acc: 0.9007632872153972
loss :0.5920097403690733
13
train acc: 0.9105424740396697
loss :0.5369261011584052
14
train acc: 0.9191885667011656
loss :0.48239906047952585
15
train acc: 0.9293142673064922
loss :0.4312946188038793
16
train acc: 0.9411393306173127
loss :0.38269632273706894
17
train acc: 0.9497175893290289
loss :0.3366098338160022
18
train acc: 0.9589390149609796
loss :0.2942858255320582
19
train acc: 0.9651250103424336
loss :0.2557390515557651
epoch: 1
test_accuracy: 0.9846677778991744
0
100
200
300
400
500
French 0 done
0
100
Spanish 0 done
0
100
200
300
400
500
German 0 done
0
100
200
300
Mandarin 0 done
0
100
English 0 done
0
100
200
300
400
500
French 1 done
0
100
Spanish 1 done
0
100
200
300
400
500
German 1 done
0
100
200
300
Mandarin 1 done
0
100
English 1 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 0 done
0
100
200
300
400
500
French 1 done
0
100
Spanish 1 done
0
100
200
300
400
500
German 1 done
0
100
200
300
Mandarin 1 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 1 done
0
100
200
300
400
500
French 2 done
0
100
Spanish 2 done
0
100
200
300
400
500
German 2 done
0
100
200
300
Mandarin 2 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 2 done
0
100
200
300
400
500
French 1 done
0
100
Spanish 1 done
0
100
200
300
Mandarin 1 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 1 done
0
100
200
300
400
500
French 2 done
0
100
Spanish 2 done
0
100
200
300
Mandarin 2 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 2 done
0
100
200
300
400
500
French 0 done
0
100
Spanish 0 done
0
100
200
300
Mandarin 0 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 0 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 0 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 1 done
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
English 2 done
0
100
200
300
400
500
German 0 done
0
100
200
300
400
500
German 1 done
0
100
200
300
400
500
German 2 done
